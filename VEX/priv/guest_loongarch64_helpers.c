
/*---------------------------------------------------------------*/
/*--- begin                       guest_loongarch64_helpers.c ---*/
/*---------------------------------------------------------------*/

/*
   This file is part of Valgrind, a dynamic binary instrumentation
   framework.

   Copyright (C) 2021-2022 Loongson Technology Corporation Limited

   This program is free software; you can redistribute it and/or
   modify it under the terms of the GNU General Public License as
   published by the Free Software Foundation; either version 2 of the
   License, or (at your option) any later version.

   This program is distributed in the hope that it will be useful, but
   WITHOUT ANY WARRANTY; without even the implied warranty of
   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
   General Public License for more details.

   You should have received a copy of the GNU General Public License
   along with this program; if not, see <http://www.gnu.org/licenses/>.

   The GNU General Public License is contained in the file COPYING.
*/

#include "libvex_basictypes.h"
#include "libvex_emnote.h"
#include "libvex_guest_loongarch64.h"
#include "libvex_ir.h"
#include "libvex.h"

#include "main_util.h"
#include "main_globals.h"
#include "guest_generic_bb_to_IR.h"
#include "guest_loongarch64_defs.h"


/* This file contains helper functions for loongarch64 guest code.
   Calls to these functions are generated by the back end. */

IRExpr* guest_loongarch64_spechelper ( const HChar * function_name,
                                       IRExpr ** args,
                                       IRStmt ** precedingStmts,
                                       Int n_precedingStmts )
{
   return NULL;
}

/* VISIBLE TO LIBVEX CLIENT */
void LibVEX_GuestLOONGARCH64_initialise ( /*OUT*/
                                          VexGuestLOONGARCH64State* vex_state )
{
   UInt i;

   /* Event check fail addr and counter. */
   vex_state->host_EvC_FAILADDR = 0;
   vex_state->host_EvC_COUNTER  = 0;

   /* CPU Registers */
   vex_state->guest_R0  = 0; /* Constant zero */
   vex_state->guest_R1  = 0; /* Return address */
   vex_state->guest_R2  = 0; /* Thread pointer */
   vex_state->guest_R3  = 0; /* Stack pointer */
   vex_state->guest_R4  = 0; /* Argument registers / Return value */
   vex_state->guest_R5  = 0;
   vex_state->guest_R6  = 0; /* Argument registers */
   vex_state->guest_R7  = 0;
   vex_state->guest_R8  = 0;
   vex_state->guest_R9  = 0;
   vex_state->guest_R10 = 0;
   vex_state->guest_R11 = 0;
   vex_state->guest_R12 = 0; /* Temporary registers */
   vex_state->guest_R13 = 0;
   vex_state->guest_R14 = 0;
   vex_state->guest_R15 = 0;
   vex_state->guest_R16 = 0;
   vex_state->guest_R17 = 0;
   vex_state->guest_R18 = 0;
   vex_state->guest_R19 = 0;
   vex_state->guest_R20 = 0;
   vex_state->guest_R21 = 0; /* Reserved */
   vex_state->guest_R22 = 0; /* Frame pointer / Static register */
   vex_state->guest_R23 = 0; /* Static registers */
   vex_state->guest_R24 = 0;
   vex_state->guest_R25 = 0;
   vex_state->guest_R26 = 0;
   vex_state->guest_R27 = 0;
   vex_state->guest_R28 = 0;
   vex_state->guest_R29 = 0;
   vex_state->guest_R30 = 0;
   vex_state->guest_R31 = 0;

   vex_state->guest_PC = 0; /* Program counter */

   /* FPU/SIMD Registers */
   for (i = 0; i < 8; i++) {
      vex_state->guest_X0[i]  = 0xffffffff;
      vex_state->guest_X1[i]  = 0xffffffff;
      vex_state->guest_X2[i]  = 0xffffffff;
      vex_state->guest_X3[i]  = 0xffffffff;
      vex_state->guest_X4[i]  = 0xffffffff;
      vex_state->guest_X5[i]  = 0xffffffff;
      vex_state->guest_X6[i]  = 0xffffffff;
      vex_state->guest_X7[i]  = 0xffffffff;
      vex_state->guest_X8[i]  = 0xffffffff;
      vex_state->guest_X9[i]  = 0xffffffff;
      vex_state->guest_X10[i] = 0xffffffff;
      vex_state->guest_X11[i] = 0xffffffff;
      vex_state->guest_X12[i] = 0xffffffff;
      vex_state->guest_X13[i] = 0xffffffff;
      vex_state->guest_X14[i] = 0xffffffff;
      vex_state->guest_X15[i] = 0xffffffff;
      vex_state->guest_X16[i] = 0xffffffff;
      vex_state->guest_X17[i] = 0xffffffff;
      vex_state->guest_X18[i] = 0xffffffff;
      vex_state->guest_X19[i] = 0xffffffff;
      vex_state->guest_X20[i] = 0xffffffff;
      vex_state->guest_X21[i] = 0xffffffff;
      vex_state->guest_X22[i] = 0xffffffff;
      vex_state->guest_X23[i] = 0xffffffff;
      vex_state->guest_X24[i] = 0xffffffff;
      vex_state->guest_X25[i] = 0xffffffff;
      vex_state->guest_X26[i] = 0xffffffff;
      vex_state->guest_X27[i] = 0xffffffff;
      vex_state->guest_X28[i] = 0xffffffff;
      vex_state->guest_X29[i] = 0xffffffff;
      vex_state->guest_X30[i] = 0xffffffff;
      vex_state->guest_X31[i] = 0xffffffff;
   }

   vex_state->guest_FCC0 = 0; /* Condition Flag Registers */
   vex_state->guest_FCC1 = 0;
   vex_state->guest_FCC2 = 0;
   vex_state->guest_FCC3 = 0;
   vex_state->guest_FCC4 = 0;
   vex_state->guest_FCC5 = 0;
   vex_state->guest_FCC6 = 0;
   vex_state->guest_FCC7 = 0;
   vex_state->guest_FCSR = 0; /* FP Control and Status Register */

   /* Various pseudo-regs mandated by Vex or Valgrind. */
   /* Emulation notes */
   vex_state->guest_EMNOTE = 0;

   /* For clflush: record start and length of area to invalidate */
   vex_state->guest_CMSTART = 0;
   vex_state->guest_CMLEN   = 0;

   /* Used to record the unredirected guest address at the start of
      a translation whose start has been redirected.  By reading
      this pseudo-register shortly afterwards, the translation can
      find out what the corresponding no-redirection address was.
      Note, this is only set for wrap-style redirects, not for
      replace-style ones. */
   vex_state->guest_NRADDR = 0;
}


/*-----------------------------------------------------------*/
/*--- Describing the loongarch64 guest state, for the     ---*/
/*--- benefit of iropt and instrumenters                  ---*/
/*-----------------------------------------------------------*/

/* Figure out if any part of the guest state contained in minoff
   .. maxoff requires precise memory exceptions.  If in doubt return
   True (but this generates significantly slower code).

   We enforce precise exns for guest SP, PC and FP.

   Only SP is needed in mode VexRegUpdSpAtMemAccess.
*/

Bool guest_loongarch64_state_requires_precise_mem_exns ( Int minoff,
                                                         Int maxoff,
                                                         VexRegisterUpdates pxControl )
{
   Int sp_min = offsetof(VexGuestLOONGARCH64State, guest_R3);
   Int sp_max = sp_min + 8 - 1;
   if ( maxoff < sp_min || minoff > sp_max ) {
      /* no overlap with sp */
      if (pxControl == VexRegUpdSpAtMemAccess)
         return False;  /* We only need to check stack pointer. */
   } else {
      return True;
   }

   Int pc_min = offsetof(VexGuestLOONGARCH64State, guest_PC);
   Int pc_max = pc_min + 8 - 1;
   if ( maxoff < pc_min || minoff > pc_max ) {
      /* no overlap with pc */
   } else {
      return True;
   }

   Int fp_min = offsetof(VexGuestLOONGARCH64State, guest_R22);
   Int fp_max = fp_min + 8 - 1;
   if ( maxoff < fp_min || minoff > fp_max ) {
      /* no overlap with fp */
   } else {
      return True;
   }

   return False;
}

#define ALWAYSDEFD64(field)                            \
   { offsetof(VexGuestLOONGARCH64State, field),        \
      (sizeof ((VexGuestLOONGARCH64State*)0)->field) }

VexGuestLayout loongarch64Guest_layout = {
   /* Total size of the guest state, in bytes. */
   .total_sizeB = sizeof(VexGuestLOONGARCH64State),
   /* Describe the stack pointer. */
   .offset_SP = offsetof(VexGuestLOONGARCH64State, guest_R3),
   .sizeof_SP = 8,
   /* Describe the frame pointer. */
   .offset_FP = offsetof(VexGuestLOONGARCH64State, guest_R22),
   .sizeof_FP = 8,
   /* Describe the instruction pointer. */
   .offset_IP = offsetof(VexGuestLOONGARCH64State, guest_PC),
   .sizeof_IP = 8,
   /* Describe any sections to be regarded by Memcheck as
      'always-defined'. */
   .n_alwaysDefd = 6,
   /* ? :(  */
   .alwaysDefd = {
                  /* 0 */ ALWAYSDEFD64(guest_R0),
                  /* 1 */ ALWAYSDEFD64(guest_PC),
                  /* 2 */ ALWAYSDEFD64(guest_EMNOTE),
                  /* 3 */ ALWAYSDEFD64(guest_CMSTART),
                  /* 4 */ ALWAYSDEFD64(guest_CMLEN),
                  /* 5 */ ALWAYSDEFD64(guest_NRADDR),
                  }
};


/*-----------------------------------------------------------*/
/*--- loongarch64 guest helpers                           ---*/
/*-----------------------------------------------------------*/

/* Claim to be the following CPU, which is probably representative of
   the earliest loongarch64 offerings.

   CPU Family          : Loongson-64bit
   Model Name          : Loongson-3A5000LL
   CPU Revision        : 0x10
   FPU Revision        : 0x00
   CPU MHz             : 2300.00
   BogoMIPS            : 4600.00
   TLB Entries         : 2112
   Address Sizes       : 48 bits physical, 48 bits virtual
   ISA                 : loongarch32 loongarch64
   Features            : cpucfg lam ual fpu lsx lasx complex crypto lvz
   Hardware Watchpoint : yes, iwatch count: 8, dwatch count: 8
*/
ULong loongarch64_calculate_cpucfg ( ULong src )
{
   ULong res;
   switch (src) {
      case 0x0:
         res = 0x0014c010;
         break;
      case 0x1:
         res = 0x03f2f2fe;
         break;
      case 0x2:
         res = 0x007ccfc7;
         break;
      case 0x3:
         res = 0x0000fcff;
         break;
      case 0x4:
         res = 0x05f5e100;
         break;
      case 0x5:
         res = 0x00010001;
         break;
      case 0x6:
         res = 0x00007f33;
         break;
      case 0x10:
         res = 0x00002c3d;
         break;
      case 0x11:
         res = 0x06080003;
         break;
      case 0x12:
         res = 0x06080003;
         break;
      case 0x13:
         res = 0x0608000f;
         break;
      case 0x14:
         res = 0x060e000f;
         break;
      default:
         res = 0x00000000;
         break;
   }
   return (ULong)(Long)(Int)res;
}

static void swap_UChar ( UChar* a, UChar* b )
{
   UChar t = *a;
   *a = *b;
   *b = t;
}

ULong loongarch64_calculate_revb_2h ( ULong src )
{
   UChar* s = (UChar*)&src;
   swap_UChar(&s[0], &s[1]);
   swap_UChar(&s[2], &s[3]);
   return (ULong)(Long)(Int)src;
}

ULong loongarch64_calculate_revb_4h ( ULong src )
{
   UChar* s = (UChar*)&src;
   swap_UChar(&s[0], &s[1]);
   swap_UChar(&s[2], &s[3]);
   swap_UChar(&s[4], &s[5]);
   swap_UChar(&s[6], &s[7]);
   return src;
}

ULong loongarch64_calculate_revb_2w ( ULong src )
{
   UChar* s = (UChar*)&src;
   swap_UChar(&s[0], &s[3]);
   swap_UChar(&s[1], &s[2]);
   swap_UChar(&s[4], &s[7]);
   swap_UChar(&s[5], &s[6]);
   return src;
}

ULong loongarch64_calculate_revb_d ( ULong src )
{
   UChar* s = (UChar*)&src;
   swap_UChar(&s[0], &s[7]);
   swap_UChar(&s[1], &s[6]);
   swap_UChar(&s[2], &s[5]);
   swap_UChar(&s[3], &s[4]);
   return src;
}

static void swap_UShort ( UShort* a, UShort* b )
{
   UShort t = *a;
   *a = *b;
   *b = t;
}

ULong loongarch64_calculate_revh_2w ( ULong src )
{
   UShort* s = (UShort*)&src;
   swap_UShort(&s[0], &s[1]);
   swap_UShort(&s[2], &s[3]);
   return src;
}

ULong loongarch64_calculate_revh_d ( ULong src )
{
   UShort* s = (UShort*)&src;
   swap_UShort(&s[0], &s[3]);
   swap_UShort(&s[1], &s[2]);
   return src;
}

static ULong bitrev ( ULong src, ULong start, ULong end )
{
   int i, j;
   ULong res = 0;
   for (i = start, j = 1; i < end; i++, j++)
      res |= ((src >> i) & 1) << (end - j);
   return res;
}

ULong loongarch64_calculate_bitrev_4b ( ULong src )
{
   ULong res = bitrev(src, 0, 8);
   res |= bitrev(src, 8, 16);
   res |= bitrev(src, 16, 24);
   res |= bitrev(src, 24, 32);
   return (ULong)(Long)(Int)res;
}

ULong loongarch64_calculate_bitrev_8b ( ULong src )
{
   ULong res = bitrev(src, 0, 8);
   res |= bitrev(src, 8, 16);
   res |= bitrev(src, 16, 24);
   res |= bitrev(src, 24, 32);
   res |= bitrev(src, 32, 40);
   res |= bitrev(src, 40, 48);
   res |= bitrev(src, 48, 56);
   res |= bitrev(src, 56, 64);
   return res;
}

ULong loongarch64_calculate_bitrev_w ( ULong src )
{
   ULong res = bitrev(src, 0, 32);
   return (ULong)(Long)(Int)res;
}

ULong loongarch64_calculate_bitrev_d ( ULong src )
{
   return bitrev(src, 0, 64);
}

static ULong crc32 ( ULong old, ULong msg, ULong width, ULong poly )
{
   int i;
   ULong new;
   if (width == 8)
      msg &= 0xff;
   else if (width == 16)
      msg &= 0xffff;
   else if (width == 32)
      msg &= 0xffffffff;
   new = (old & 0xffffffff) ^ msg;
   for (i = 0; i < width; i++) {
      if (new & 1)
         new = (new >> 1) ^ poly;
      else
         new >>= 1;
   }
   return new;
}

ULong loongarch64_calculate_crc ( ULong old, ULong msg, ULong len )
{
   ULong res = crc32(old, msg, len, 0xedb88320);
   return (ULong)(Long)(Int)res;
}

ULong loongarch64_calculate_crcc ( ULong old, ULong msg, ULong len )
{
   ULong res = crc32(old, msg, len, 0x82f63b78);
   return (ULong)(Long)(Int)res;
}

ULong loongarch64_calculate_fclass_s ( ULong src )
{
   UInt f = src;
   Bool sign = toBool(f >> 31);
   if ((f & 0x7fffffff) == 0x7f800000) {
      return sign ? 1 << 2 : 1 << 6;
   } else if ((f & 0x7fffffff) == 0) {
      return sign ? 1 << 5 : 1 << 9;
   } else if ((f & 0x7f800000) == 0) {
      return sign ? 1 << 4 : 1 << 8;
   } else if ((f & ~(1 << 31)) > 0x7f800000) {
      return ((UInt)(f << 1) >= 0xff800000) ? 1 << 1 : 1 << 0;
   } else {
      return sign ? 1 << 3 : 1 << 7;
   }
}

ULong loongarch64_calculate_fclass_d ( ULong src )
{
   ULong f = src;
   Bool sign = toBool(f >> 63);
   if ((f & 0x7fffffffffffffffULL) == 0x7ff0000000000000ULL) {
      return sign ? 1 << 2 : 1 << 6;
   } else if ((f & 0x7fffffffffffffffULL) == 0) {
      return sign ? 1 << 5 : 1 << 9;
   } else if ((f & 0x7ff0000000000000ULL) == 0) {
      return sign ? 1 << 4 : 1 << 8;
   } else if ((f & ~(1ULL << 63)) > 0x7ff0000000000000ULL) {
      return ((f << 1) >= 0xfff0000000000000ULL) ? 1 << 1 : 1 << 0;
   } else {
      return sign ? 1 << 3 : 1 << 7;
   }
}

ULong loongarch64_calculate_negative_id ( ULong insSz, ULong sHi, ULong sLo )
{
   V128 src;
   UInt i;

   src.w64[1] = sHi;
   src.w64[0] = sLo;

   switch (insSz) {
      case 0b00: {
         for (i = 0; i < 16; i++) {
            if ((Char)src.w8[i] < 0)
               break;
         }
         break;
      }
      case 0b01: {
         for (i = 0; i < 8; i++) {
            if ((Short)src.w16[i] < 0)
               break;
         }
         break;
      }
      default:
         vassert(0);
         break;
   }

   return (ULong)i;
}

#if defined(__loongarch__)
#define ASM_VOLATILE_UNARY(inst)                         \
   __asm__ volatile("movfcsr2gr $s0, $r0         \n\t"   \
                    "movgr2fcsr $r2, $zero       \n\t"   \
                    #inst"      $f24, %1         \n\t"   \
                    "movfcsr2gr %0, $r2          \n\t"   \
                    "movgr2fcsr $r0, $s0         \n\t"   \
                    : "=r" (fcsr2)                       \
                    : "f" (src1)                         \
                    : "$s0", "$f24"                      \
                   )

#define ASM_VOLATILE_BINARY(inst)                        \
   __asm__ volatile("movfcsr2gr $s0, $r0         \n\t"   \
                    "movgr2fcsr $r2, $zero       \n\t"   \
                    #inst"      $f24, %1, %2     \n\t"   \
                    "movfcsr2gr %0, $r2          \n\t"   \
                    "movgr2fcsr $r0, $s0         \n\t"   \
                    : "=r" (fcsr2)                       \
                    : "f" (src1), "f" (src2)             \
                    : "$s0", "$f24"                      \
                   )

#define ASM_VOLATILE_TRINARY(inst)                       \
   __asm__ volatile("movfcsr2gr $s0, $r0         \n\t"   \
                    "movgr2fcsr $r2, $zero       \n\t"   \
                    #inst"      $f24, %1, %2, %3 \n\t"   \
                    "movfcsr2gr %0, $r2          \n\t"   \
                    "movgr2fcsr $r0, $s0         \n\t"   \
                    : "=r" (fcsr2)                       \
                    : "f" (src1), "f" (src2), "f" (src3) \
                    : "$s0", "$f24"                      \
                   )

#define ASM_VOLATILE_FCMP(inst)                          \
   __asm__ volatile("movfcsr2gr $s0, $r0         \n\t"   \
                    "movgr2fcsr $r2, $zero       \n\t"   \
                    #inst"      $fcc0, %1, %2    \n\t"   \
                    "movfcsr2gr %0, $r0          \n\t"   \
                    "movgr2fcsr $r0, $s0         \n\t"   \
                    : "=r" (fcsr2)                       \
                    : "f" (src1), "f" (src2)             \
                    : "$s0", "$fcc0"                     \
                   )
#endif

/* Calculate FCSR and return whether an exception needs to be thrown */
ULong loongarch64_calculate_FCSR ( enum fpop op, ULong src1,
                                   ULong src2, ULong src3 )
{
   UInt fcsr2 = 0;
#if defined(__loongarch__)
   switch (op) {
      case FADD_S:
         ASM_VOLATILE_BINARY(fadd.s);
         break;
      case FADD_D:
         ASM_VOLATILE_BINARY(fadd.d);
         break;
      case FSUB_S:
         ASM_VOLATILE_BINARY(fsub.s);
         break;
      case FSUB_D:
         ASM_VOLATILE_BINARY(fsub.d);
         break;
      case FMUL_S:
         ASM_VOLATILE_BINARY(fmul.s);
         break;
      case FMUL_D:
         ASM_VOLATILE_BINARY(fmul.d);
         break;
      case FDIV_S:
         ASM_VOLATILE_BINARY(fdiv.s);
         break;
      case FDIV_D:
         ASM_VOLATILE_BINARY(fdiv.d);
         break;
      case FMADD_S:
         ASM_VOLATILE_TRINARY(fmadd.s);
         break;
      case FMADD_D:
         ASM_VOLATILE_TRINARY(fmadd.d);
         break;
      case FMSUB_S:
         ASM_VOLATILE_TRINARY(fmsub.s);
         break;
      case FMSUB_D:
         ASM_VOLATILE_TRINARY(fmsub.d);
         break;
      case FNMADD_S:
         ASM_VOLATILE_TRINARY(fnmadd.s);
         break;
      case FNMADD_D:
         ASM_VOLATILE_TRINARY(fnmadd.d);
         break;
      case FNMSUB_S:
         ASM_VOLATILE_TRINARY(fnmsub.s);
         break;
      case FNMSUB_D:
         ASM_VOLATILE_TRINARY(fnmsub.s);
         break;
      case FMAX_S:
         ASM_VOLATILE_BINARY(fmax.s);
         break;
      case FMAX_D:
         ASM_VOLATILE_BINARY(fmax.d);
         break;
      case FMIN_S:
         ASM_VOLATILE_BINARY(fmin.s);
         break;
      case FMIN_D:
         ASM_VOLATILE_BINARY(fmin.d);
         break;
      case FMAXA_S:
         ASM_VOLATILE_BINARY(fmaxa.s);
         break;
      case FMAXA_D:
         ASM_VOLATILE_BINARY(fmaxa.d);
         break;
      case FMINA_S:
         ASM_VOLATILE_BINARY(fmina.s);
         break;
      case FMINA_D:
         ASM_VOLATILE_BINARY(fmina.s);
         break;
      case FABS_S:
         ASM_VOLATILE_UNARY(fabs.s);
         break;
      case FABS_D:
         ASM_VOLATILE_UNARY(fabs.d);
         break;
      case FNEG_S:
         ASM_VOLATILE_UNARY(fneg.s);
         break;
      case FNEG_D:
         ASM_VOLATILE_UNARY(fneg.d);
         break;
      case FSQRT_S:
         ASM_VOLATILE_UNARY(fsqrt.s);
         break;
      case FSQRT_D:
         ASM_VOLATILE_UNARY(fsqrt.d);
         break;
      case FRECIP_S:
         ASM_VOLATILE_UNARY(frecip.s);
         break;
      case FRECIP_D:
         ASM_VOLATILE_UNARY(frecip.d);
         break;
      case FRSQRT_S:
         ASM_VOLATILE_UNARY(frsqrt.s);
         break;
      case FRSQRT_D:
         ASM_VOLATILE_UNARY(frsqrt.d);
         break;
      case FSCALEB_S:
         ASM_VOLATILE_BINARY(fscaleb.s);
         break;
      case FSCALEB_D:
         ASM_VOLATILE_BINARY(fscaleb.d);
         break;
      case FLOGB_S:
         ASM_VOLATILE_UNARY(flogb.s);
         break;
      case FLOGB_D:
         ASM_VOLATILE_UNARY(flogb.d);
         break;
      case FCMP_CAF_S:
         ASM_VOLATILE_FCMP(fcmp.caf.s);
         break;
      case FCMP_CAF_D:
         ASM_VOLATILE_FCMP(fcmp.caf.d);
         break;
      case FCMP_SAF_S:
         ASM_VOLATILE_FCMP(fcmp.saf.s);
         break;
      case FCMP_SAF_D:
         ASM_VOLATILE_FCMP(fcmp.saf.d);
         break;
      case FCMP_CLT_S:
         ASM_VOLATILE_FCMP(fcmp.clt.s);
         break;
      case FCMP_CLT_D:
         ASM_VOLATILE_FCMP(fcmp.clt.d);
         break;
      case FCMP_SLT_S:
         ASM_VOLATILE_FCMP(fcmp.slt.s);
         break;
      case FCMP_SLT_D:
         ASM_VOLATILE_FCMP(fcmp.slt.d);
         break;
      case FCMP_CEQ_S:
         ASM_VOLATILE_FCMP(fcmp.ceq.s);
         break;
      case FCMP_CEQ_D:
         ASM_VOLATILE_FCMP(fcmp.ceq.d);
         break;
      case FCMP_SEQ_S:
         ASM_VOLATILE_FCMP(fcmp.seq.s);
         break;
      case FCMP_SEQ_D:
         ASM_VOLATILE_FCMP(fcmp.seq.d);
         break;
      case FCMP_CLE_S:
         ASM_VOLATILE_FCMP(fcmp.cle.s);
         break;
      case FCMP_CLE_D:
         ASM_VOLATILE_FCMP(fcmp.cle.d);
         break;
      case FCMP_SLE_S:
         ASM_VOLATILE_FCMP(fcmp.sle.s);
         break;
      case FCMP_SLE_D:
         ASM_VOLATILE_FCMP(fcmp.sle.d);
         break;
      case FCMP_CUN_S:
         ASM_VOLATILE_FCMP(fcmp.cun.s);
         break;
      case FCMP_CUN_D:
         ASM_VOLATILE_FCMP(fcmp.cun.d);
         break;
      case FCMP_SUN_S:
         ASM_VOLATILE_FCMP(fcmp.sun.s);
         break;
      case FCMP_SUN_D:
         ASM_VOLATILE_FCMP(fcmp.sun.d);
         break;
      case FCMP_CULT_S:
         ASM_VOLATILE_FCMP(fcmp.cult.s);
         break;
      case FCMP_CULT_D:
         ASM_VOLATILE_FCMP(fcmp.cult.d);
         break;
      case FCMP_SULT_S:
         ASM_VOLATILE_FCMP(fcmp.sult.s);
         break;
      case FCMP_SULT_D:
         ASM_VOLATILE_FCMP(fcmp.sult.d);
         break;
      case FCMP_CUEQ_S:
         ASM_VOLATILE_FCMP(fcmp.cueq.s);
         break;
      case FCMP_CUEQ_D:
         ASM_VOLATILE_FCMP(fcmp.cueq.d);
         break;
      case FCMP_SUEQ_S:
         ASM_VOLATILE_FCMP(fcmp.sueq.s);
         break;
      case FCMP_SUEQ_D:
         ASM_VOLATILE_FCMP(fcmp.sueq.d);
         break;
      case FCMP_CULE_S:
         ASM_VOLATILE_FCMP(fcmp.cule.s);
         break;
      case FCMP_CULE_D:
         ASM_VOLATILE_FCMP(fcmp.cule.d);
         break;
      case FCMP_SULE_S:
         ASM_VOLATILE_FCMP(fcmp.sule.s);
         break;
      case FCMP_SULE_D:
         ASM_VOLATILE_FCMP(fcmp.sule.d);
         break;
      case FCMP_CNE_S:
         ASM_VOLATILE_FCMP(fcmp.cne.s);
         break;
      case FCMP_CNE_D:
         ASM_VOLATILE_FCMP(fcmp.cne.d);
         break;
      case FCMP_SNE_S:
         ASM_VOLATILE_FCMP(fcmp.sne.s);
         break;
      case FCMP_SNE_D:
         ASM_VOLATILE_FCMP(fcmp.sne.d);
         break;
      case FCMP_COR_S:
         ASM_VOLATILE_FCMP(fcmp.cor.s);
         break;
      case FCMP_COR_D:
         ASM_VOLATILE_FCMP(fcmp.cor.d);
         break;
      case FCMP_SOR_S:
         ASM_VOLATILE_FCMP(fcmp.sor.s);
         break;
      case FCMP_SOR_D:
         ASM_VOLATILE_FCMP(fcmp.sor.d);
         break;
      case FCMP_CUNE_S:
         ASM_VOLATILE_FCMP(fcmp.cune.s);
         break;
      case FCMP_CUNE_D:
         ASM_VOLATILE_FCMP(fcmp.cune.d);
         break;
      case FCMP_SUNE_S:
         ASM_VOLATILE_FCMP(fcmp.sune.s);
         break;
      case FCMP_SUNE_D:
         ASM_VOLATILE_FCMP(fcmp.sune.d);
         break;
      case FCVT_S_D:
         ASM_VOLATILE_UNARY(fcvt.s.d);
         break;
      case FCVT_D_S:
         ASM_VOLATILE_UNARY(fcvt.d.s);
         break;
      case FTINTRM_W_S:
         ASM_VOLATILE_UNARY(ftintrm.w.s);
         break;
      case FTINTRM_W_D:
         ASM_VOLATILE_UNARY(ftintrm.w.d);
         break;
      case FTINTRM_L_S:
         ASM_VOLATILE_UNARY(ftintrm.l.s);
         break;
      case FTINTRM_L_D:
         ASM_VOLATILE_UNARY(ftintrm.l.d);
         break;
      case FTINTRP_W_S:
         ASM_VOLATILE_UNARY(ftintrp.w.s);
         break;
      case FTINTRP_W_D:
         ASM_VOLATILE_UNARY(ftintrp.w.d);
         break;
      case FTINTRP_L_S:
         ASM_VOLATILE_UNARY(ftintrp.l.s);
         break;
      case FTINTRP_L_D:
         ASM_VOLATILE_UNARY(ftintrp.l.d);
         break;
      case FTINTRZ_W_S:
         ASM_VOLATILE_UNARY(ftintrz.w.s);
         break;
      case FTINTRZ_W_D:
         ASM_VOLATILE_UNARY(ftintrz.w.d);
         break;
      case FTINTRZ_L_S:
         ASM_VOLATILE_UNARY(ftintrz.l.s);
         break;
      case FTINTRZ_L_D:
         ASM_VOLATILE_UNARY(ftintrz.l.d);
         break;
      case FTINTRNE_W_S:
         ASM_VOLATILE_UNARY(ftintrne.w.s);
         break;
      case FTINTRNE_W_D:
         ASM_VOLATILE_UNARY(ftintrne.w.d);
         break;
      case FTINTRNE_L_S:
         ASM_VOLATILE_UNARY(ftintrne.l.s);
         break;
      case FTINTRNE_L_D:
         ASM_VOLATILE_UNARY(ftintrne.l.d);
         break;
      case FTINT_W_S:
         ASM_VOLATILE_UNARY(ftint.w.s);
         break;
      case FTINT_W_D:
         ASM_VOLATILE_UNARY(ftint.w.d);
         break;
      case FTINT_L_S:
         ASM_VOLATILE_UNARY(ftint.l.s);
         break;
      case FTINT_L_D:
         ASM_VOLATILE_UNARY(ftint.l.d);
         break;
      case FFINT_S_W:
         ASM_VOLATILE_UNARY(ffint.s.w);
         break;
      case FFINT_D_W:
         ASM_VOLATILE_UNARY(ffint.d.w);
         break;
      case FFINT_S_L:
         ASM_VOLATILE_UNARY(ffint.s.l);
         break;
      case FFINT_D_L:
         ASM_VOLATILE_UNARY(ffint.d.l);
         break;
      case FRINT_S:
         ASM_VOLATILE_UNARY(frint.s);
         break;
      case FRINT_D:
         ASM_VOLATILE_UNARY(frint.d);
         break;
      default:
         break;
   }
#endif
   return (ULong)fcsr2;
}


/*---------------------------------------------------------------*/
/*--- end                         guest_loongarch64_helpers.c ---*/
/*---------------------------------------------------------------*/
